# -*- coding: utf-8 -*-
"""
è¿è¡Œæ¸¸æˆæ•°æ®æŠ“å–
æ”¯æŒæ–°çš„ä¸‰APIæŠ“å–æµç¨‹
"""
import asyncio
import argparse
import sys
import logging
from pathlib import Path
from typing import List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# é…ç½®æ—¥å¿—ï¼šå®Œå…¨ç¦ç”¨ SQLAlchemy çš„æ—¥å¿—è¾“å‡º
logging.getLogger("sqlalchemy.engine").setLevel(logging.ERROR)
logging.getLogger("sqlalchemy.pool").setLevel(logging.ERROR)
logging.getLogger("sqlalchemy.dialects").setLevel(logging.ERROR)
logging.getLogger("sqlalchemy.orm").setLevel(logging.ERROR)
# ç¦ç”¨æ‰€æœ‰ SQLAlchemy ç›¸å…³çš„æ—¥å¿—
logging.getLogger("sqlalchemy").setLevel(logging.ERROR)

from app.database import SessionLocal
from app.services.crawler_service import CrawlerService


def parse_rank_ids(rank_str: str) -> List[int]:
    """è§£ærankIdåˆ—è¡¨ï¼Œæ”¯æŒèŒƒå›´å¦‚ 1-100 æˆ–é€—å·åˆ†éš”å¦‚ 1,2,3"""
    rank_ids = []
    for part in rank_str.split(","):
        part = part.strip()
        if "-" in part:
            start, end = part.split("-", 1)
            rank_ids.extend(range(int(start), int(end) + 1))
        else:
            rank_ids.append(int(part))
    return sorted(set(rank_ids))


async def main():
    parser = argparse.ArgumentParser(description="æ¸¸æˆæ•°æ®æŠ“å–å·¥å…·")
    parser.add_argument("--all", action="store_true", help="æŠ“å–æ‰€æœ‰æ•°æ®ï¼ˆæ—§APIï¼‰")
    parser.add_argument("--offset", type=int, default=0, help="åç§»é‡ï¼ˆæ—§APIï¼‰")
    
    # æ–°APIå‚æ•°
    parser.add_argument("--ranks", type=str, default="1-100", help="è¦æŠ“å–çš„rankIdåˆ—è¡¨ï¼Œæ ¼å¼: 1-100 æˆ– 1,2,3")
    parser.add_argument("--from-json", action="store_true", help="ä»JSONæ–‡ä»¶è¯»å–å¹¶å†™å…¥æ•°æ®åº“")
    parser.add_argument("--fetch-details", action="store_true", help="å¯¹å·²æŠ“å–çš„rankæ•°æ®ï¼Œè°ƒç”¨pageå’Œscore APIè·å–è¯¦æƒ…")
    parser.add_argument("--reviews-only", action="store_true", help="ä»gamesè¡¨è¯»å–æ‰€æœ‰æ¸¸æˆï¼Œé‡æ–°æŠ“å–å¹¶ä¿å­˜æ‰€æœ‰reviews")
    parser.add_argument("--concurrency", type=int, default=5, help="å¹¶å‘æ•°ï¼ˆç”¨äº--reviews-onlyï¼‰ï¼Œé»˜è®¤5")
    parser.add_argument("--delay", type=float, default=2.0, help="æ‰¹æ¬¡å»¶è¿Ÿï¼ˆç§’ï¼Œç”¨äº--reviews-onlyï¼‰ï¼Œé»˜è®¤2.0")
    parser.add_argument("--limit", type=int, help="é™åˆ¶å¤„ç†çš„æ¸¸æˆæ•°é‡ï¼ˆç”¨äº--reviews-onlyæµ‹è¯•æˆ–æ—§APIï¼‰")
    
    args = parser.parse_args()
    
    service = CrawlerService()
    
    try:
        # reviews-only æµç¨‹ï¼ˆä¼˜å…ˆå¤„ç†ï¼‰
        if args.reviews_only:
            print("=" * 60)
            print("å¼€å§‹é‡æ–°æŠ“å–æ‰€æœ‰ reviews...")
            print("=" * 60)
            
            db = SessionLocal()
            try:
                stats = await service.crawl_all_reviews(
                    db,
                    concurrency=args.concurrency,
                    delay=args.delay,
                    limit=args.limit
                )
                
                print(f"\n{'='*60}")
                print(f"âœ“ Reviews æŠ“å–å®Œæˆï¼")
                print(f"  æ€»æ¸¸æˆæ•°: {stats['total_games']}")
                print(f"  æˆåŠŸ: {stats['success_count']} ä¸ª")
                print(f"  å¤±è´¥: {stats['failed_count']} ä¸ª")
                print(f"  ä¿å­˜è¯„è®ºæ€»æ•°: {stats['total_reviews']} æ¡")
                print(f"{'='*60}\n")
            finally:
                db.close()
                await service.crawler.close()
            return
        
        # æ–°APIæµç¨‹
        if args.from_json:
            print("ä»JSONæ–‡ä»¶è¯»å–æ•°æ®å¹¶å†™å…¥æ•°æ®åº“...")
            rank_ids = parse_rank_ids(args.ranks)
            games_data = service.load_and_parse_json_files(rank_ids)
            print(f"ä»JSONåŠ è½½äº† {len(games_data)} ä¸ªæ¸¸æˆ")
            
            db = SessionLocal()
            try:
                if args.fetch_details:
                    print("è·å–æ¸¸æˆè¯¦æƒ…ï¼ˆpageå’Œscore APIï¼‰...")
                    print(f"ä½¿ç”¨å¹¶è¡Œå¤„ç†ï¼Œå¹¶å‘æ•°: 5ï¼Œæ‰¹æ¬¡å»¶è¿Ÿ: 2ç§’")
                    print(f"æŠ“ä¸€æ‰¹å†™ä¸€æ‰¹æ¨¡å¼ï¼Œå®æ—¶æ˜¾ç¤ºå†™å…¥ç»Ÿè®¡\n")
                    
                    # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ï¼Œåªåœ¨å¼€å§‹æ—¶è¾“å‡ºä¸€æ¬¡
                    from sqlalchemy import inspect
                    inspector = inspect(db.bind)
                    # æ˜ç¡®æŒ‡å®šæ£€æŸ¥ public schema
                    tables = set(inspector.get_table_names(schema='public'))
                    missing_tables = []
                    required_tables = {
                        "game_rank_relations": "005_create_game_relations_tables.sql",
                        "game_prices": "005_create_game_relations_tables.sql",
                        "game_media_scores": "005_create_game_relations_tables.sql",
                        "reviews": "006_create_reviews_table.sql",
                    }
                    for table, sql_file in required_tables.items():
                        if table not in tables:
                            missing_tables.append((table, sql_file))
                    
                    if missing_tables:
                        print(f"âš ï¸  ä»¥ä¸‹è¡¨ä¸å­˜åœ¨ï¼Œå°†è·³è¿‡ç›¸å…³æ•°æ®ä¿å­˜ï¼š")
                        sql_files = {}
                        for table, sql_file in missing_tables:
                            if sql_file not in sql_files:
                                sql_files[sql_file] = []
                            sql_files[sql_file].append(table)
                        
                        for sql_file, table_list in sql_files.items():
                            print(f"  - {', '.join(table_list)} (éœ€è¦æ‰§è¡Œ: database/init/{sql_file})")
                        print(f"\nğŸ’¡ æç¤º: è¿è¡Œ 'python3 scripts/check_tables.py' æ£€æŸ¥æ‰€æœ‰è¡¨çŠ¶æ€\n")
                    
                    # ç´¯è®¡ç»Ÿè®¡
                    total_stats = {
                        "saved_count": 0,
                        "updated_count": 0,
                        "failed_count": 0,
                        "relations_stats": {
                            "rank_relations": 0,
                            "prices": 0,
                            "media_scores": 0,
                            "reviews": 0
                        }
                    }
                    
                    async def on_batch_complete(batch_games, batch_start, batch_end):
                        """æ‰¹æ¬¡å®Œæˆå›è°ƒï¼šå†™å…¥æ•°æ®åº“å¹¶æ˜¾ç¤ºç»Ÿè®¡"""
                        batch_stats = service.save_games_to_db(db, batch_games, show_progress=False)
                        
                        # ç´¯åŠ ç»Ÿè®¡
                        total_stats["saved_count"] += batch_stats["saved_count"]
                        total_stats["updated_count"] += batch_stats["updated_count"]
                        total_stats["failed_count"] += batch_stats["failed_count"]
                        for key in total_stats["relations_stats"]:
                            total_stats["relations_stats"][key] += batch_stats["relations_stats"][key]
                        
                        # æ˜¾ç¤ºæ‰¹æ¬¡ç»Ÿè®¡ï¼ˆç®€æ´æ ¼å¼ï¼‰
                        print(f"[æ‰¹æ¬¡ {batch_start + 1}-{batch_end}] âœ“ å†™å…¥å®Œæˆ | "
                              f"æ¸¸æˆ: +{batch_stats['saved_count']} â†‘{batch_stats['updated_count']} âœ—{batch_stats['failed_count']} | "
                              f"å…³è”: æ¦œå•{batch_stats['relations_stats']['rank_relations']} "
                              f"ä»·æ ¼{batch_stats['relations_stats']['prices']} "
                              f"è¯„åˆ†{batch_stats['relations_stats']['media_scores']} "
                              f"è¯„è®º{batch_stats['relations_stats']['reviews']} | "
                              f"ç´¯è®¡: æ¸¸æˆ{total_stats['saved_count'] + total_stats['updated_count']} "
                              f"å…³è”{sum(total_stats['relations_stats'].values())}")
                    
                    await service.fetch_game_details_batch(
                        games_data, 
                        concurrency=5, 
                        delay_between_batches=2.0,
                        on_batch_complete=on_batch_complete
                    )
                    
                    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
                    print(f"\n{'='*60}")
                    print(f"âœ“ å…¨éƒ¨å®Œæˆï¼æœ€ç»ˆç»Ÿè®¡:")
                    print(f"  æ¸¸æˆ: æ–°å¢ {total_stats['saved_count']}, æ›´æ–° {total_stats['updated_count']}, å¤±è´¥ {total_stats['failed_count']}")
                    print(f"  å…³è”æ•°æ®:")
                    print(f"    - æ¦œå•å…³è”: {total_stats['relations_stats']['rank_relations']}")
                    print(f"    - ä»·æ ¼ä¿¡æ¯: {total_stats['relations_stats']['prices']}")
                    print(f"    - åª’ä½“è¯„åˆ†: {total_stats['relations_stats']['media_scores']}")
                    print(f"    - è¯„è®º: {total_stats['relations_stats']['reviews']}")
                    print(f"{'='*60}\n")
                else:
                    # ä¸è·å–è¯¦æƒ…ï¼Œç›´æ¥å†™å…¥
                    stats = service.save_games_to_db(db, games_data)
                    print(f"âœ“ å®Œæˆï¼å…±ä¿å­˜ {stats['saved_count']} æ¡æ¸¸æˆæ•°æ®")
            finally:
                db.close()
        
        elif args.fetch_details:
            print("ä»…è·å–æ¸¸æˆè¯¦æƒ…ï¼ˆéœ€è¦å…ˆæœ‰JSONæ–‡ä»¶ï¼‰...")
            rank_ids = parse_rank_ids(args.ranks)
            games_data = service.load_and_parse_json_files(rank_ids)
            
            print(f"è·å– {len(games_data)} ä¸ªæ¸¸æˆçš„è¯¦æƒ…...")
            print(f"ä½¿ç”¨å¹¶è¡Œå¤„ç†ï¼Œå¹¶å‘æ•°: 5ï¼Œæ‰¹æ¬¡å»¶è¿Ÿ: 2ç§’")
            await service.fetch_game_details_batch(
                games_data, 
                concurrency=5, 
                delay_between_batches=2.0
            )
            
            # ä¿å­˜æ›´æ–°åçš„æ•°æ®åˆ°JSON
            for rank_id in rank_ids:
                # è¿™é‡Œå¯ä»¥é‡æ–°ç»„ç»‡æ•°æ®ä¿å­˜ï¼Œæš‚æ—¶è·³è¿‡
                pass
            
            print("âœ“ å®Œæˆï¼")
        
        else:
            # æŠ“å–æ¦œå•æ•°æ®åˆ°JSON
            rank_ids = parse_rank_ids(args.ranks)
            print(f"å‡†å¤‡æŠ“å–æ¦œå•æ•°æ® (å…± {len(rank_ids)} ä¸ªæ¦œå•)...")
            results = await service.crawl_all_ranks(rank_ids)
            
            print(f"\næ•°æ®å·²ä¿å­˜åˆ° backend/data/rank_*.json")
            print(f"ä¸‹ä¸€æ­¥: ä½¿ç”¨ --from-json å‚æ•°å°†æ•°æ®å†™å…¥æ•°æ®åº“")
        
        # æ—§APIæµç¨‹ï¼ˆä¿æŒå…¼å®¹ï¼‰
        if args.all or args.limit:
            db = SessionLocal()
            try:
                if args.all:
                    print("å¼€å§‹æŠ“å–æ‰€æœ‰æ¸¸æˆæ•°æ®ï¼ˆæ—§APIï¼‰...")
                    total = await service.crawl_all(db)
                    print(f"âœ“ å®Œæˆï¼å…±ä¿å­˜ {total} æ¡æ¸¸æˆæ•°æ®")
                elif args.limit:
                    print(f"å¼€å§‹æŠ“å– {args.limit} æ¡æ¸¸æˆæ•°æ®ï¼ˆæ—§APIï¼‰...")
                    saved = await service.crawl_and_save(db, limit=args.limit, offset=args.offset)
                    print(f"âœ“ å®Œæˆï¼å…±ä¿å­˜ {saved} æ¡æ¸¸æˆæ•°æ®")
            finally:
                db.close()
                await service.crawler.close()
        
        if not any([args.from_json, args.fetch_details, args.all, args.limit, args.reviews_only]):
            print("è¯·æŒ‡å®šæ“ä½œå‚æ•°")
            parser.print_help()
            
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        await service.crawler.close()


if __name__ == "__main__":
    asyncio.run(main())

